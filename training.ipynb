{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_dirs = [\"data/naodevils/images\", \"data/bhuman\"]\n",
    "data_dirs = [\"data/naodevils/spqr_autolabel_and_manual_train_patchified.csv\", \"data/bhuman/b-alls-2019_train_patchified.csv\"]\n",
    "val_dirs = [\"data/naodevils/spqr_manual_val_patchified.csv\", \"data/bhuman/b-alls-2019_val_patchified.csv\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BallDataset(Dataset):\n",
    "    def __init__(self, data_dirs, images_dirs, transform=None):\n",
    "        self.data_dirs = data_dirs\n",
    "        self.images_dirs = images_dirs\n",
    "        self.transform = transform\n",
    "        \n",
    "        dfs = []\n",
    "        for data_dir, images_dir in zip(data_dirs, images_dirs):\n",
    "            df = pd.read_csv(data_dir)\n",
    "            df['images_dir'] = images_dir  \n",
    "            dfs.append(df)\n",
    "        self.df = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        if self.transform:\n",
    "            self.dimension = transform.transforms[0].size[0]\n",
    "        else:\n",
    "            self.dimension = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        img_dir = row[\"images_dir\"]\n",
    "        \n",
    "        img = Image.open(os.path.join(img_dir, row[\"image\"])).convert(\"L\")\n",
    "        \n",
    "        patch_dim = (row[\"patch_x\"], row[\"patch_y\"], row[\"patch_x\"] + row[\"patch_size\"], row[\"patch_y\"] + row[\"patch_size\"])\n",
    "        patch = img.crop(patch_dim)\n",
    "        \n",
    "        resize_ratio = self.dimension / row[\"patch_size\"] if self.dimension else 1\n",
    "        \n",
    "        if row[\"patch_contains_ball\"]:\n",
    "            x = (row[\"center_x\"] - row[\"patch_x\"]) * resize_ratio\n",
    "            y = (row[\"center_y\"] - row[\"patch_y\"]) * resize_ratio\n",
    "            r = row[\"radius\"] * resize_ratio\n",
    "        else:\n",
    "            x, y, r = float('nan'), float('nan'), float('nan')\n",
    "        \n",
    "        if self.transform:\n",
    "            patch = self.transform(patch)\n",
    "        \n",
    "        patch = patch * 255.0\n",
    "\n",
    "        #assert pd.isna(x) or x <= 32, (row[\"patch_x\"], row[\"patch_size\"], row[\"center_x\"], x)\n",
    "\n",
    "        return patch, row[\"patch_contains_ball\"], {\"x\": x, \"y\": y, \"r\": r}\n",
    "    \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.patches import Circle\n",
    "\n",
    "dataset = BallDataset(data_dirs, image_dirs, transform)\n",
    "validation = BallDataset(val_dirs, image_dirs, transform)\n",
    "\n",
    "batch_size = 256\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "sample = next(iter(data_loader))\n",
    "\n",
    "num_images = 5\n",
    "\n",
    "ig, axes = plt.subplots(2, num_images, figsize=(num_images * 3, 3))\n",
    "\n",
    "for i in range(num_images):\n",
    "    axes[0, i].imshow(sample[0][i][0], cmap='gray')\n",
    "    if sample[1][i]:\n",
    "        c = Circle((sample[2][\"x\"][i].item(),sample[2][\"y\"][i].item()), sample[2][\"r\"][i].item())\n",
    "        c.set_facecolor((1.0,0.,0.,0.4))\n",
    "        axes[0, i].add_patch(c)\n",
    "    axes[0, i].axis('off')\n",
    "    axes[1, i].text(0.5, 0, f'Contains Ball: {sample[1][i]}', horizontalalignment='center')  # Add text under each image\n",
    "    axes[1, i].axis('off')  # Turn off axis for text\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, use_residual=False):\n",
    "        super(DepthwiseSeparableConv, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.use_residual = use_residual\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.depthwise(x)\n",
    "        z = self.pointwise(z)\n",
    "        z = self.bn(z)\n",
    "        z = self.relu(z) + x if self.use_residual else self.relu(z)\n",
    "        return z\n",
    "\n",
    "\n",
    "class BallPerceptor(nn.Module):\n",
    "    def __init__(self, backbone_size = 9):\n",
    "        super(BallPerceptor, self).__init__()\n",
    "        \n",
    "        self.conv1 = DepthwiseSeparableConv(1, 8, stride=1)\n",
    "        self.conv2 = DepthwiseSeparableConv(8, 16, stride=1)\n",
    "        self.conv3 = DepthwiseSeparableConv(16, 32, stride=1)\n",
    "        \n",
    "        backbone_layers = []\n",
    "        for _ in range(backbone_size):\n",
    "            backbone_layers.append(DepthwiseSeparableConv(32, 32, stride=1,use_residual=True))\n",
    "\n",
    "        self.backbone = nn.Sequential(*backbone_layers)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    " \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32 * 4 * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "        self.segmenter = nn.Sequential(\n",
    "            nn.Linear(32 * 4 * 4, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 3), # x, y, r\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.conv1(x))\n",
    "        x = self.pool(self.conv2(x))\n",
    "        x = self.pool(self.conv3(x))\n",
    "        \n",
    "        x = self.backbone(x)\n",
    "\n",
    "        feature_map = x.view(-1, 32 * 4 * 4)\n",
    "\n",
    "        classification = self.classifier(feature_map) if self.training else torch.sigmoid(self.classifier(feature_map))\n",
    "        segmentation = self.segmenter(feature_map)\n",
    "        return classification, segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "class Circular_DIoU(_Loss):\n",
    "    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean') -> None:\n",
    "        super().__init__(size_average, reduce, reduction)\n",
    "\n",
    "    def intersectionArea(self, X1, Y1, R1, X2, Y2, R2):\n",
    "        d = torch.sqrt(((X2 - X1) * (X2 - X1)) + ((Y2 - Y1) * (Y2 - Y1)))\n",
    "\n",
    "        # Case 1: no overlap \n",
    "        mask_1 = d > R1 + R2\n",
    "\n",
    "        # Case 2: One circle is fully within the other\n",
    "        mask_2 = ( d <= (R1 - R2) ) & ( R1 >= R2 )\n",
    "        mask_3 = ( d <= (R2 - R1) ) & ( R2 > R1 )\n",
    "\n",
    "        # Case 3: Partial overlap\n",
    "        mask_else = ~(mask_1 | mask_2 | mask_3)\n",
    "\n",
    "        intersection_area = torch.zeros_like(d, dtype=torch.float32)\n",
    "\n",
    "        intersection_area[mask_2] = torch.pi * R2[mask_2] * R2[mask_2]\n",
    "        intersection_area[mask_3] = torch.pi * R1[mask_3] * R1[mask_3]\n",
    "\n",
    "        alpha = torch.acos(((R1[mask_else] * R1[mask_else]) + (d[mask_else] * d[mask_else]) - (R2[mask_else] * R2[mask_else])) / (2 * R1[mask_else] * d[mask_else])) * 2\n",
    "        beta = torch.acos(((R2[mask_else] * R2[mask_else]) + (d[mask_else] * d[mask_else]) - (R1[mask_else] * R1[mask_else])) / (2 * R2[mask_else] * d[mask_else])) * 2\n",
    "        \n",
    "        a1 = (0.5 * beta * R2[mask_else] * R2[mask_else] ) - (0.5 * R2[mask_else] * R2[mask_else] * torch.sin(beta))\n",
    "        a2 = (0.5 * alpha * R1[mask_else] * R1[mask_else]) - (0.5 * R1[mask_else] * R1[mask_else] * torch.sin(alpha))\n",
    "\n",
    "        intersection_area[mask_else] = a1 + a2\n",
    "\n",
    "        return intersection_area\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        a1 = torch.pi * torch.pow(input[:,2], 2)\n",
    "        a2 = torch.pi * torch.pow(target[:,2], 2)\n",
    "\n",
    "        a1inta2 = self.intersectionArea(input[:,0], input[:,1], input[:,2], target[:,0], target[:,1], target[:,2])\n",
    "\n",
    "        a1una2 = a1 + a2 - a1inta2\n",
    "\n",
    "        center_diff = input[:,:2] - target[:,:2]\n",
    "\n",
    "        center_dist = torch.norm(center_diff, dim=-1)\n",
    "        D = torch.pow(center_dist, 2)/torch.pow(center_dist + input[:,2] + target[:,2], 2)\n",
    "\n",
    "        DIoU = 1 - a1inta2/a1una2 + D\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return DIoU.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return DIoU.sum()\n",
    "        else:\n",
    "            return DIoU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersectionArea(X1, Y1, R1, X2, Y2, R2):\n",
    "        d = torch.sqrt(((X2 - X1) * (X2 - X1)) + ((Y2 - Y1) * (Y2 - Y1)))\n",
    "\n",
    "        # Case 1: no overlap \n",
    "        mask_1 = d > R1 + R2\n",
    "\n",
    "        # Case 2: One circle is fully within the other\n",
    "        mask_2 = ( d <= (R1 - R2) ) & ( R1 >= R2 )\n",
    "        mask_3 = ( d <= (R2 - R1) ) & ( R2 > R1 )\n",
    "\n",
    "        # Case 3: Partial overlap\n",
    "        mask_else = ~(mask_1 | mask_2 | mask_3)\n",
    "\n",
    "        intersection_area = torch.zeros_like(d, dtype=torch.float32)\n",
    "\n",
    "        intersection_area[mask_2] = torch.pi * R2[mask_2] * R2[mask_2]\n",
    "        intersection_area[mask_3] = torch.pi * R1[mask_3] * R1[mask_3]\n",
    "\n",
    "        alpha = torch.acos(((R1[mask_else] * R1[mask_else]) + (d[mask_else] * d[mask_else]) - (R2[mask_else] * R2[mask_else])) / (2 * R1[mask_else] * d[mask_else])) * 2\n",
    "        beta = torch.acos(((R2[mask_else] * R2[mask_else]) + (d[mask_else] * d[mask_else]) - (R1[mask_else] * R1[mask_else])) / (2 * R2[mask_else] * d[mask_else])) * 2\n",
    "\n",
    "        a1 = (0.5 * beta * R2[mask_else] * R2[mask_else] ) - (0.5 * R2[mask_else] * R2[mask_else] * torch.sin(beta))\n",
    "        a2 = (0.5 * alpha * R1[mask_else] * R1[mask_else]) - (0.5 * R1[mask_else] * R1[mask_else] * torch.sin(alpha))\n",
    "\n",
    "        intersection_area[mask_else] = a1 + a2\n",
    "\n",
    "        return intersection_area\n",
    "\n",
    "def IoU(input, target):\n",
    "        a1 = torch.pi * torch.pow(input[:,2], 2)\n",
    "        a2 = torch.pi * torch.pow(target[:,2], 2)\n",
    "        a1inta2 = intersectionArea(input[:,0], input[:,1], input[:,2], target[:,0], target[:,1], target[:,2])\n",
    "        iou = a1inta2/ (a1 + a2 - a1inta2)\n",
    "        center_diff = input[:,:2] - target[:,:2]\n",
    "        center_dist = torch.norm(center_diff, dim=-1)\n",
    "        D = torch.pow(center_dist, 2)/torch.pow(center_dist + input[:,2] + target[:,2], 2)\n",
    "        return iou.mean() , D.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "best_loss = float(\"inf\")\n",
    "#best_loss = 1.8359"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model: nn.Module, detection_weight: float, best_loss: float):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in validation_loader:\n",
    "            images, labels = data[0].to(device), data[1].float().to(device)\n",
    "            segmentation_gt = torch.stack((data[2][\"x\"], data[2][\"y\"], data[2][\"r\"]), dim=1).to(device).to(torch.float)\n",
    "            classification_mask = labels == 1.\n",
    "\n",
    "            classification, segmentation = model(images)\n",
    "\n",
    "            val_loss += classification_criterion(classification, labels.unsqueeze(1))                    \n",
    "            if classification_mask.any():\n",
    "                val_loss += detection_weight * segmentation_criterion(segmentation[classification_mask], segmentation_gt[classification_mask])\n",
    "\n",
    "            predicted = torch.round(classification)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.unsqueeze(1)).sum().item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    model.train()\n",
    "    accuracy = 100 * correct / total\n",
    "    val_loss /= total / batch_size\n",
    "\n",
    "    # Compute F1-score\n",
    "    f1 = f1_score(all_labels, all_predictions, average='binary')\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    model.train()\n",
    "    return val_loss, accuracy, f1*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from datetime import datetime\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model: nn.Module, detection_weight: float, best_loss: float, path:str):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in validation_loader:\n",
    "            images, labels = data[0].to(device), data[1].float().to(device)\n",
    "            segmentation_gt = torch.stack((data[2][\"x\"], data[2][\"y\"], data[2][\"r\"]), dim=1).to(device).to(torch.float)\n",
    "            classification_mask = labels == 1.\n",
    "\n",
    "            classification, segmentation = model(images)\n",
    "\n",
    "            val_loss += classification_criterion(classification, labels.unsqueeze(1))                    \n",
    "            if classification_mask.any():\n",
    "                val_loss += detection_weight * segmentation_criterion(segmentation[classification_mask], segmentation_gt[classification_mask])\n",
    "                iou, d = IoU(segmentation[classification_mask], segmentation_gt[classification_mask])\n",
    "            predicted = torch.round(classification)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.unsqueeze(1)).sum().item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    model.train()\n",
    "    accuracy = 100 * correct / total\n",
    "    val_loss /= total / batch_size\n",
    "\n",
    "    # Compute F1-score\n",
    "    f1 = f1_score(all_labels, all_predictions, average='binary')\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), path)\n",
    "\n",
    "    model.train()\n",
    "    return val_loss, accuracy, f1*100, iou, d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "dataset = BallDataset(data_dirs, image_dirs, transform)\n",
    "validation = BallDataset(val_dirs, image_dirs, transform)\n",
    "\n",
    "batch_size = 256\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "validation_loader = DataLoader(validation, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model, weights_path=None):\n",
    "    if weights_path is None:\n",
    "        return model\n",
    "    weights = torch.load(weights_path)\n",
    "    model.load_state_dict(weights, strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "p_weight = torch.tensor([0.6]).to(device)\n",
    "classification_criterion = nn.BCEWithLogitsLoss(pos_weight=p_weight)\n",
    "segmentation_criterion = Circular_DIoU()\n",
    "\n",
    "num_epochs = 5\n",
    "iterations = 5 #how many iterations in total\n",
    "log_rate = 100\n",
    "eval_rate = 300\n",
    "\n",
    "\n",
    "##### CHANGE HERE TO TEST DIFFERENT MODELS##########\n",
    "\n",
    "path = \"results/Maml.pt\" #path to save model\n",
    "best_loss_file = path.replace('.pt', '.txt') #path to save results\n",
    "weights_path = 'weights/Maml.pt' #path for loading weights\n",
    "\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder \"results\" already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_name = \"results\"\n",
    "\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "    print(f'Folder \"{folder_name}\" created.')\n",
    "else:\n",
    "    print(f'Folder \"{folder_name}\" already exists.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_weight = 5\n",
    "best_loss = float(\"inf\")\n",
    "tot_loss = 0\n",
    "logs = [] \n",
    "\n",
    "for it in range(iterations):\n",
    "    best_loss = float(\"inf\")\n",
    "    model = BallPerceptor()\n",
    "    model = load_weights(model,weights_path)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in tqdm.tqdm(enumerate(data_loader, 0), total=len(dataset)//batch_size):\n",
    "            inputs, labels = data[0].to(device), data[1].float().to(device)\n",
    "            segmentation_gt = torch.stack((data[2][\"x\"], data[2][\"y\"], data[2][\"r\"]), dim=1).to(device).to(torch.float)\n",
    "\n",
    "            classification_mask = labels == 1.\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            classification, segmentation = model(inputs)\n",
    "            loss = classification_criterion(classification, labels.unsqueeze(1))\n",
    "            if classification_mask.any():\n",
    "                loss = detection_weight*segmentation_criterion(segmentation[classification_mask], segmentation_gt[classification_mask]) + loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (i+1) % log_rate == 0: \n",
    "                print('[Epoch %d, Batch %5d] Training loss: %.5f' %\n",
    "                    (epoch + 1, i + 1, running_loss / log_rate))\n",
    "                running_loss = 0.0\n",
    "        \n",
    "            if (i + 1) % eval_rate == 0:\n",
    "                val_loss, accuracy, f1, iou, d = validate(model, detection_weight, best_loss, path)\n",
    "                log = '[Iteration %d, Epoch %d] Accuracy: %f --- F1: %f --- Loss: %f --- IoU: %f --- D: %f' % \\\n",
    "                      (it + 1, epoch + 1, accuracy, f1, val_loss, iou, d)\n",
    "                print(log)\n",
    "                logs.append(log)\n",
    "\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    log = 'Epoch: %d --- Accuracy: %f --- F1: %f  --- Loss: %f --- IoU: %f --- D: %f' % (epoch + 1, accuracy, f1, val_loss, iou, d)\n",
    "                    \n",
    "\n",
    "        val_loss, accuracy, f1, iou, d = validate(model, detection_weight, best_loss, path)\n",
    "        print('Epoch: %d --- Accuracy: %f --- F1: %f --- Loss: %f --- IoU: %f --- D: %f' % (epoch + 1, accuracy, f1, val_loss, iou, d))\n",
    "    tot_loss += best_loss\n",
    "\n",
    "avg_loss = tot_loss / iterations\n",
    "with open(best_loss_file, 'a') as f:\n",
    "    for l in logs:\n",
    "        f.write(l + '\\n')\n",
    "    f.write(f'Average loss is: {avg_loss}\\n')\n",
    "\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model: nn.Module, detection_weight: float, best_loss: float, path:str, lower, upper):\n",
    "    model = model.to(device)  # Ensure model is on the right device\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    # For mAP calculation\n",
    "    all_confidences = []\n",
    "    all_has_ball = []  # Whether the image has a ball or not\n",
    "    \n",
    "    # Track overall metrics across batches\n",
    "    all_samples = 0\n",
    "    total_iou = 0\n",
    "    total_d = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in validation_loader:\n",
    "            images, labels = data[0].to(device), data[1].float().to(device)\n",
    "            segmentation_gt = torch.stack((data[2][\"x\"], data[2][\"y\"], data[2][\"r\"]), dim=1).to(device).to(torch.float)\n",
    "            classification_mask = labels == 1.\n",
    "            \n",
    "            classification, segmentation = model(images)\n",
    "            val_loss += classification_criterion(classification, labels.unsqueeze(1))\n",
    "            \n",
    "            # Store classification confidences\n",
    "            confidences = classification.sigmoid().squeeze(1) if hasattr(classification, 'sigmoid') else classification.squeeze(1)\n",
    "            all_confidences.extend(confidences.cpu().numpy())\n",
    "            all_has_ball.extend(labels.cpu().numpy())\n",
    "            \n",
    "            if classification_mask.any():\n",
    "                val_loss += detection_weight * segmentation_criterion(segmentation[classification_mask], segmentation_gt[classification_mask])\n",
    "                \n",
    "                # Calculate IoU for this batch\n",
    "                batch_iou, batch_d = IoU(segmentation[classification_mask], segmentation_gt[classification_mask])\n",
    "                \n",
    "                # Accumulate for averaging later\n",
    "                total_iou += batch_iou.item() * classification_mask.sum().item()\n",
    "                total_d += batch_d.item() * classification_mask.sum().item()\n",
    "                all_samples += classification_mask.sum().item()\n",
    "            \n",
    "            predicted = torch.round(classification)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.squeeze(1) == labels).sum().item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.squeeze(1).cpu().numpy())\n",
    "    \n",
    "    # Calculate average IoU and D\n",
    "    avg_iou = total_iou / max(all_samples, 1)\n",
    "    avg_d = total_d / max(all_samples, 1)\n",
    "    \n",
    "    # Calculate mAP50-95 using a modified approach that works with our available data\n",
    "    map_50_95 = calculate_map_50_95(all_confidences, all_has_ball,lower,upper)\n",
    "    \n",
    "    # Original metrics\n",
    "    accuracy = 100 * correct / total\n",
    "    val_loss /= total / batch_size\n",
    "    # Compute F1-score\n",
    "    f1 = f1_score(all_labels, all_predictions, average='binary')\n",
    "    \n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), path)\n",
    "    \n",
    "    model.train()\n",
    "    return val_loss, accuracy, f1*100, avg_iou, avg_d, map_50_95\n",
    "\n",
    "def calculate_map_50_95(confidences, has_ball,lower, upper):\n",
    "    \"\"\"\n",
    "    Calculate an approximation of mAP50-95 based on classification scores.\n",
    "    This is a simplified version that works with binary classification.\n",
    "    \n",
    "    Args:\n",
    "        confidences: List of confidence scores\n",
    "        has_ball: List of ground truth labels (1 = has ball, 0 = no ball)\n",
    "    \n",
    "    Returns:\n",
    "        mAP50-95: Approximated Mean Average Precision\n",
    "    \"\"\"\n",
    "    confidences = np.array(confidences)\n",
    "    has_ball = np.array(has_ball)\n",
    "    \n",
    "    # Define confidence thresholds to simulate IoU thresholds\n",
    "    thresholds = np.arange(lower, upper+0.5, 0.5)\n",
    "    \n",
    "    aps = []\n",
    "    for threshold in thresholds:\n",
    "        # For each threshold, calculate predictions\n",
    "        predictions = confidences >= threshold\n",
    "        \n",
    "        # Number of positive ground truths\n",
    "        n_positives = np.sum(has_ball == 1)\n",
    "        if n_positives == 0:\n",
    "            aps.append(0)\n",
    "            continue\n",
    "            \n",
    "        # Sort by confidence score (descending)\n",
    "        indices = np.argsort(-confidences)\n",
    "        sorted_predictions = predictions[indices]\n",
    "        sorted_has_ball = has_ball[indices]\n",
    "        \n",
    "        # Calculate cumulative TP and FP\n",
    "        tp_cumsum = np.cumsum(sorted_predictions & (sorted_has_ball == 1))\n",
    "        fp_cumsum = np.cumsum(sorted_predictions & (sorted_has_ball == 0))\n",
    "        \n",
    "        # Calculate precision and recall\n",
    "        precision = tp_cumsum / (tp_cumsum + fp_cumsum + 1e-10)\n",
    "        recall = tp_cumsum / n_positives\n",
    "        \n",
    "        # Add sentinel values for interpolation\n",
    "        precision = np.concatenate(([1.0], precision))\n",
    "        recall = np.concatenate(([0.0], recall))\n",
    "        \n",
    "        # Ensure precision is non-increasing for proper interpolation\n",
    "        for i in range(len(precision) - 1, 0, -1):\n",
    "            precision[i-1] = max(precision[i-1], precision[i])\n",
    "        \n",
    "        # Calculate AP using all-point interpolation (101-point method)\n",
    "        ap = 0\n",
    "        for r in np.linspace(0, 1, 101):\n",
    "            indices = recall >= r\n",
    "            if indices.any():\n",
    "                ap += np.max(precision[indices]) / 101\n",
    "        \n",
    "        aps.append(ap)\n",
    "    \n",
    "    # Return the mean AP across all thresholds\n",
    "    return np.mean(aps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BallPerceptor(\n",
       "  (conv1): DepthwiseSeparableConv(\n",
       "    (depthwise): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (pointwise): Conv2d(1, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (conv2): DepthwiseSeparableConv(\n",
       "    (depthwise): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
       "    (pointwise): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (conv3): DepthwiseSeparableConv(\n",
       "    (depthwise): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "    (pointwise): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (backbone): Sequential(\n",
       "    (0): DepthwiseSeparableConv(\n",
       "      (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): DepthwiseSeparableConv(\n",
       "      (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): DepthwiseSeparableConv(\n",
       "      (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (3): DepthwiseSeparableConv(\n",
       "      (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (4): DepthwiseSeparableConv(\n",
       "      (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (5): DepthwiseSeparableConv(\n",
       "      (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (6): DepthwiseSeparableConv(\n",
       "      (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (7): DepthwiseSeparableConv(\n",
       "      (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (8): DepthwiseSeparableConv(\n",
       "      (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       "  (segmenter): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=32, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelBase = BallPerceptor()\n",
    "weights_path = \"results/base.pt\"\n",
    "modelBase = load_weights(modelBase,weights_path)\n",
    "modelBase.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BallPerceptor(\n",
       "  (conv1): DepthwiseSeparableConv(\n",
       "    (depthwise): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (pointwise): Conv2d(1, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (conv2): DepthwiseSeparableConv(\n",
       "    (depthwise): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
       "    (pointwise): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (conv3): DepthwiseSeparableConv(\n",
       "    (depthwise): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "    (pointwise): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (backbone): Sequential(\n",
       "    (0): DepthwiseSeparableConv(\n",
       "      (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): DepthwiseSeparableConv(\n",
       "      (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): DepthwiseSeparableConv(\n",
       "      (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (3): DepthwiseSeparableConv(\n",
       "      (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (4): DepthwiseSeparableConv(\n",
       "      (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (5): DepthwiseSeparableConv(\n",
       "      (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (6): DepthwiseSeparableConv(\n",
       "      (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (7): DepthwiseSeparableConv(\n",
       "      (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (8): DepthwiseSeparableConv(\n",
       "      (depthwise): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "      (pointwise): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       "  (segmenter): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=128, out_features=32, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=32, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelMaml = BallPerceptor()\n",
    "weights_path = \"results/Maml.pt\"\n",
    "modelMaml = load_weights(modelMaml,weights_path)\n",
    "modelMaml.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47612664631297746\n",
      "0.4852580555131791\n"
     ]
    }
   ],
   "source": [
    "_, _, _, _, _, map1 = validate(modelBase, 5, 1, path, lower = 0.5, upper = 0.95)\n",
    "_, _, _, _, _, map2 = validate(modelMaml, 5, 1, path, lower = 0.5, upper = 0.95)\n",
    "print(map1)\n",
    "print(map2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4399976539810195\n",
      "0.4569739269588459\n"
     ]
    }
   ],
   "source": [
    "_, _, _, _, _, map1 = validate(modelBase, 5, 1, path, lower = 0.6, upper = 0.95)\n",
    "_, _, _, _, _, map2 = validate(modelMaml, 5, 1, path, lower = 0.6, upper = 0.95)\n",
    "print(map1)\n",
    "print(map2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38500830001951386\n",
      "0.4150158780166046\n"
     ]
    }
   ],
   "source": [
    "_, _, _, _, _, map1 = validate(modelBase, 5, 1, path, lower = 0.7, upper = 0.95)\n",
    "_, _, _, _, _, map2 = validate(modelMaml, 5, 1, path, lower = 0.7, upper = 0.95)\n",
    "print(map1)\n",
    "print(map2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009900990099009901\n",
      "0.009900990099009901\n"
     ]
    }
   ],
   "source": [
    "_, _, _, _, _, map1 = validate(modelBase, 5, 1, path, lower = 0.8, upper = 0.95)\n",
    "_, _, _, _, _, map2 = validate(modelMaml, 5, 1, path, lower = 0.8, upper = 0.95)\n",
    "print(map1)\n",
    "print(map2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProjVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
